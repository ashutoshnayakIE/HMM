{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to HMM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short guide through to hidden Markov Models. While HMM is widely used, the concepts for this workbook have been adopted by using the following resources:\n",
    "<ul>\n",
    "    <li>https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf</li>\n",
    "    <li>Example: https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9</li>\n",
    "    <li>API: https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GaussianHMM</li>\n",
    "    <li>Code Adopted from: https://github.com/jiaeyan/Hidden-Markov-Model</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HMMs, we observe some outcome variables (<b>$O_1,O_2...,O_T$</b>) which are driven from latent (hidden) variables. The probability of getting that outcome from one of the hidden state is called emission probability (and is denoted by <b>B</b>). In each time step, the latent state may change. However, this change depends only on the previous hidden state (hence it is called Markov model). These states transition with probability given by <b>A</b>. The model starts from an initial state. The initial state is denoted by <b>$\\pi$</b>. \n",
    "\n",
    "The input parameters include - how many hidden states we would like to have in our model (denoted by <b>N</b>). Another parameter is the number of possible outcomes (in a discrete setting) and it is denoted by <b>M</b>. Thus a HMM model is denoted by the tuple $\\lambda = (A,B,\\pi)$. The notations are explained in the Figure below.\n",
    "\n",
    "<img src=\"HMM.png\" alt=\"HMM\" width=\"628\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Three HMM problems</h2>\n",
    "\n",
    "HMM are used to model three types of problems:\n",
    "<ol>\n",
    "    <li>Evaluation problem: If we know the model <b>(A,B,$\\pi$)</b>, what is the probability of observing a given sequence?</li>\n",
    "    <li>Decoding problem: If we know the model <b>(A,B,$\\pi$)</b>, what is the best sequence of the hidden states that explain the sequence of the observations?</li>\n",
    "    <li>Learning problem: How to estimate the value of <b>(A,B,$\\pi$)</b> if we observe a given sequence of observations (or what model led to the generation of the given sequence). This is supervised HMM model.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation  : [1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
      "hidden states: [2, 2, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0, 2, 0, 0, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Consider a problem from https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781\n",
    "# outcome: hot (0) or cold (1)\n",
    "# hidden states : snow, rain, sunshine\n",
    "pi = np.array([0,0.2,0.8])\n",
    "A  = np.array([[0.3,0.3,0.4],[0.1,0.45,0.45],[0.2,0.3,0.5]])\n",
    "B  = np.array([[1,0],[0.8,0.2],[0.3,0.7]])\n",
    "M  = 2 #(hot or cold)\n",
    "N  = 3 # snow, rain, sunshine\n",
    "T  = 20\n",
    "\n",
    "# simulate the walk based on transition probability\n",
    "# since this is generated based on actual probabilities, this state should have high probability of being observed\n",
    "s = np.random.choice(3,1,p=pi)[0]\n",
    "o = np.random.choice(2,1,p=B[s])[0]\n",
    "S = [s]\n",
    "O = [o]\n",
    "\n",
    "for t in range(T-1):\n",
    "    s = np.random.choice(3,1,p=A[s])[0]\n",
    "    o = np.random.choice(2,1,p=B[s])[0]\n",
    "    S.append(s)\n",
    "    O.append(o)\n",
    "print('observation  :',O)\n",
    "print('hidden states:',S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation problem: Forward algorithm and Backward algorithm</h2>\n",
    "\n",
    "In this problem, we evaluate the probability of the observation sequence being observed in real life (if we already know the HMM). We first use forward algorithm and then use backward algorithm to show that we get same probability using both algorithms.\n",
    "\n",
    "\n",
    "<img src=\"Evaluation.png\" alt=\"forward-backward algorithm\" width=\"828\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>forward algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7890062156250946e-06"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define alpha as the probability of partially observing the sequence upto t with state qt at time t\n",
    "# initialization\n",
    "alpha      = np.zeros((N,T))\n",
    "alpha[:,0] = B[:,O[0]]*pi.T\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in range(1,T):\n",
    "    for s2 in range(N):\n",
    "        for s1 in range(N):\n",
    "            alpha[s2,t] += alpha[s1,t-1]*A[s1,s2]*B[s2,O[t]]\n",
    "            \n",
    "# final probability\n",
    "prob_of_observing = np.sum(alpha[:,-1])\n",
    "prob_of_observing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>backward algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7890062156250956e-06"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define beta as the probability of partially observing the sequence from t+1 with state qt at time t\n",
    "# initialization\n",
    "beta       = np.zeros((N,T))\n",
    "beta[:,-1] = 1   # because sequence is satisfied from T+1 onwards\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in reversed(range(T-1)):\n",
    "    for s1 in range(N):\n",
    "        for s2 in range(N):\n",
    "            beta[s1,t] += beta[s2,t+1]*A[s1,s2]*B[s2,O[t+1]]\n",
    "            \n",
    "# final probability\n",
    "prob_of_observing = np.sum(beta[:,0]*B[:,O[0]]*pi.T)\n",
    "prob_of_observing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decoding problem</h2>\n",
    "\n",
    "In the decoding problem, we aim to find the best sequence for hidden states that led to the generation of the observation sequence as observed. The outline is shown below as we code for viterbi algorithm next. It is very similar to forward algorithm, just that in place of sum, we find the maximum.\n",
    "\n",
    "<img src=\"Decoding.png\" alt=\"viterbi algorithm\" width=\"828\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Viterbi algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original hidden state:     [2, 2, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0, 2, 0, 0, 2, 1, 1]\n",
      "hidden state from viterbi: [2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# in viterbi algorithm, we still know the HMM model (A,B,pi), we just dont know the hidden state sequence\n",
    "# the objective is to find the hidden state sequence to make inference. \n",
    "\n",
    "delta      = np.zeros((N,T))\n",
    "delta[:,0] = B[:,O[0]]*pi.T\n",
    "psi        = np.zeros((N,T))    # keeps a track of the best sequence\n",
    "psi        = [0]*T\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in range(1,T):\n",
    "    for s2 in range(N):\n",
    "        vals = [0]*N\n",
    "        for s1 in range(N):\n",
    "            vals[s1] = delta[s1,t-1]*A[s1,s2]*B[s2,O[t]]\n",
    "        psi[t-1]    = np.argmax(vals)\n",
    "        delta[s2,t] = vals[psi[t-1]]\n",
    "            \n",
    "# optimal hidden state sequence\n",
    "print('original hidden state:    ', S)\n",
    "print('hidden state from viterbi:',psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning problem: EM algorithm</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
