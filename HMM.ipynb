{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to HMM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short guide through to hidden Markov Models. While HMM is widely used, the concepts for this workbook have been adopted by using the following resources:\n",
    "<ul>\n",
    "    <li><a href=\"https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\">HMM Tutorial</a></li>\n",
    "    <li><a href =\"https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9\"> An easy example</a></li>\n",
    "    <li><a href=\"https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GaussianHMM\">hmmlearn Python API</a></li>\n",
    "    <li><a href=\"https://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/\">Derivation of HMM for coding</a></li>\n",
    "</ul>\n",
    "    \n",
    "The objective of this tutorial is to help in understanding of HMM and how to code it. It is not the objective to build an API or to code efficiently (but rather a lot for loops for better understanding of the underpinnings of the algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HMMs, we observe some outcome variables (<b>$O_1,O_2...,O_T$</b>) which are driven from latent (hidden) variables. The probability of getting that outcome from one of the hidden state is called emission probability (and is denoted by <b>B</b>). In each time step, the latent state may change. However, this change depends only on the previous hidden state (hence it is called Markov model). These states transition with probability given by <b>A</b>. The model starts from an initial state. The initial state is denoted by <b>$\\pi$</b>. \n",
    "\n",
    "The input parameters include - how many hidden states we would like to have in our model (denoted by <b>N</b>). Another parameter is the number of possible outcomes (in a discrete setting) and it is denoted by <b>M</b>. Thus a HMM model is denoted by the tuple $\\lambda = (A,B,\\pi)$. The notations are explained in the Figure below.\n",
    "\n",
    "<img src=\"HMM.png\" alt=\"HMM\" width=\"628\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Three HMM problems</h2>\n",
    "\n",
    "HMM are used to model three types of problems:\n",
    "<ol>\n",
    "    <li>Evaluation problem: If we know the model <b>(A,B,$\\pi$)</b>, what is the probability of observing a given sequence?</li>\n",
    "    <li>Decoding problem: If we know the model <b>(A,B,$\\pi$)</b>, what is the best sequence of the hidden states that explain the sequence of the observations?</li>\n",
    "    <li>Learning problem: How to estimate the value of <b>(A,B,$\\pi$)</b> if we observe a given sequence of observations (or what model led to the generation of the given sequence). This is supervised HMM model.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation  : [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "hidden states: [2, 2, 0, 0, 2, 0, 2, 0, 1, 1, 2, 1, 1, 1, 2, 2, 0, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Consider a problem from https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781\n",
    "# outcome: hot (0) or cold (1)\n",
    "# hidden states : snow, rain, sunshine\n",
    "np.random.seed(42)\n",
    "pi = np.array([0,0.2,0.8])\n",
    "A  = np.array([[0.3,0.3,0.4],[0.1,0.45,0.45],[0.2,0.3,0.5]])\n",
    "B  = np.array([[1,0],[0.8,0.2],[0.3,0.7]])\n",
    "M  = 2 #(hot or cold)\n",
    "N  = 3 # snow, rain, sunshine\n",
    "T  = 20\n",
    "\n",
    "# simulate the walk based on transition probability\n",
    "# since this is generated based on actual probabilities, this state should have high probability of being observed\n",
    "s = np.random.choice(3,1,p=pi)[0]\n",
    "o = np.random.choice(2,1,p=B[s])[0]\n",
    "S = [s]\n",
    "O = [o]\n",
    "\n",
    "for t in range(T-1):\n",
    "    s = np.random.choice(3,1,p=A[s])[0]\n",
    "    o = np.random.choice(2,1,p=B[s])[0]\n",
    "    S.append(s)\n",
    "    O.append(o)\n",
    "print('observation  :',O)\n",
    "print('hidden states:',S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation problem: Forward algorithm and Backward algorithm</h2>\n",
    "\n",
    "In this problem, we evaluate the probability of the observation sequence being observed in real life (if we already know the HMM). We first use forward algorithm and then use backward algorithm to show that we get same probability using both algorithms.\n",
    "\n",
    "\n",
    "<img src=\"Evaluation.png\" alt=\"forward-backward algorithm\" width=\"828\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>forward algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.287718892764278e-06"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define alpha as the probability of partially observing the sequence upto t with state qt at time t\n",
    "# initialization\n",
    "alpha      = np.zeros((N,T))\n",
    "alpha[:,0] = B[:,O[0]]*pi.T\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in range(1,T):\n",
    "    for s2 in range(N):\n",
    "        for s1 in range(N):\n",
    "            alpha[s2,t] += alpha[s1,t-1]*A[s1,s2]*B[s2,O[t]]\n",
    "            \n",
    "# final probability\n",
    "prob_of_observing = np.sum(alpha[:,-1])\n",
    "prob_of_observing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>backward algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.287718892764281e-06"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define beta as the probability of partially observing the sequence from t+1 with state qt at time t\n",
    "# initialization\n",
    "beta       = np.zeros((N,T))\n",
    "beta[:,-1] = 1   # because sequence is satisfied from T+1 onwards\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in reversed(range(T-1)):\n",
    "    for s1 in range(N):\n",
    "        for s2 in range(N):\n",
    "            beta[s1,t] += beta[s2,t+1]*A[s1,s2]*B[s2,O[t+1]]\n",
    "            \n",
    "# final probability\n",
    "prob_of_observing = np.sum(beta[:,0]*B[:,O[0]]*pi.T)\n",
    "prob_of_observing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decoding problem</h2>\n",
    "\n",
    "In the decoding problem, we aim to find the best sequence for hidden states that led to the generation of the observation sequence as observed. The outline is shown below as we code for viterbi algorithm next. It is very similar to forward algorithm, just that in place of sum, we find the maximum.\n",
    "\n",
    "<img src=\"Decoding.png\" alt=\"viterbi algorithm\" width=\"828\" height=\"628\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Viterbi algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original hidden state:     [2 2 0 0 2 0 2 0 1 1 2 1 1 1 2 2 0 2 1 2]\n",
      "hidden state from viterbi: [2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# in viterbi algorithm, we still know the HMM model (A,B,pi), we just dont know the hidden state sequence\n",
    "# the objective is to find the hidden state sequence to make inference. \n",
    "\n",
    "delta      = np.zeros((N,T))\n",
    "delta[:,0] = B[:,O[0]]*pi.T    \n",
    "psi        = np.array([0]*T)   # keeps a track of the best sequence\n",
    "\n",
    "# recursion            (s2 is the next state and s1 is the previous state)\n",
    "for t in range(1,T):\n",
    "    for s2 in range(N):\n",
    "        vals = [0]*N\n",
    "        for s1 in range(N):\n",
    "            vals[s1] = delta[s1,t-1]*A[s1,s2]*B[s2,O[t]]\n",
    "        psi[t-1]    = np.argmax(vals)\n",
    "        delta[s2,t] = vals[psi[t-1]]\n",
    "\n",
    "# last state is directly based on the last observation\n",
    "psi[t] = np.argmax(B[:,O[T-1]])\n",
    "# optimal hidden state sequence\n",
    "print('original hidden state:    ', S)\n",
    "print('hidden state from viterbi:',psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning problem: EM algorithm</h2>\n",
    "\n",
    "Learning problem is considered to be the tough problem of finding the actual HMM parameters (A,B, $\\pi$). This problem is also called supervised learning problem as parameters are obtained by looking into the outcomes. The values of A,B and $\\pi$ can be obtained using gradient based optimization methods. We can also use other optimization routines likes bayesian optimization to find the parameters. Optimization methods are based on finding the error between true observation and observation prediction from the optimization routing. EM algorithm is more statistical flavor as it is based on maximizing the maximum likelihood. \n",
    "\n",
    "Here, we will use EM algorithm for finding the HMM parameters. HMM uses <b>Baum-Welch</b> algorithm which is a special case of Expectation maximization algorithm.\n",
    "\n",
    "\n",
    "<img src=\"Learning.png\" alt=\"baum welch algorithm\" width=\"828\" height=\"628\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing A and B\n",
    "# assuming we know the information on actual value of pi \n",
    "# if there are multiple sequences, it will just be the average of the number of times a state exist in the first step\n",
    "\n",
    "A = np.ones((N,N))/N\n",
    "B = np.ones((N,M))/M\n",
    "\n",
    "\n",
    "# starting with the iteration\n",
    "# we can create a function for forward and backward algorithm but for the sake of ease of following through, just copy pasted \n",
    "# here. We will use the same code as before to calculate eta and gamma\n",
    "\n",
    "\n",
    "for iter in range(10):\n",
    "    # ------------- expectation step ------------------\n",
    "    # use A and B to obtain gamma and eta\n",
    "    \n",
    "    # alpha (from forward algorithm)\n",
    "    alpha      = np.zeros((N,T))\n",
    "    alpha[:,0] = B[:,O[0]]*pi.T\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        for s2 in range(N):\n",
    "            for s1 in range(N):\n",
    "                alpha[s2,t] += alpha[s1,t-1]*A[s1,s2]*B[s2,O[t]]\n",
    "    \n",
    "    # beta (from backward algorithm)\n",
    "    beta       = np.zeros((N,T))\n",
    "    beta[:,-1] = 1   # because sequence is satisfied from T+1 onwards\n",
    "    \n",
    "    for t in reversed(range(T-1)):\n",
    "        for s1 in range(N):\n",
    "            for s2 in range(N):\n",
    "                beta[s1,t] += beta[s2,t+1]*A[s1,s2]*B[s2,O[t+1]]\n",
    "    \n",
    "    # emission based on the known observation sequence\n",
    "    emission = np.zeros((N,T))\n",
    "    for t in range(T):\n",
    "        emission[:,t] = B[:,O[t]]\n",
    "        \n",
    "    # gamma\n",
    "    gamma = np.multiply(alpha,beta)\n",
    "    gamma /= np.sum(gamma,axis=0)\n",
    "    \n",
    "    # eta\n",
    "    eta   = np.zeros((N,N,T))\n",
    "    for t in range(T):\n",
    "        sum_t = 0\n",
    "        for s1 in range(N):\n",
    "            if t == T-1:\n",
    "                eta[s1,:,t] = alpha[s1,t]*1*A[s1,:]*1\n",
    "                sum_t       += eta[s1,s2,t]\n",
    "            else:\n",
    "                eta[s1,:,t] = alpha[s1,t]*beta[:,t+1]*A[s1,:]*emission[:,t+1]\n",
    "                sum_t       += eta[s1,:,t]\n",
    "\n",
    "        eta[:,:,t]/= np.sum(sum_t)\n",
    "                \n",
    "    # ------------- maximization step ------------------\n",
    "    # use eta and gamma to obtain A and B\n",
    "\n",
    "    A = np.sum(eta,axis=2)\n",
    "    A/= np.sum(A,axis=0)\n",
    "    \n",
    "    B = np.zeros((N,M))\n",
    "    for m in range(M):\n",
    "        hh = np.array(O)==m\n",
    "        hh = hh.reshape(1,T)  # counting as how many times a particular obersvation was made\n",
    "        B[:,m:m+1] = np.sum(np.multiply(gamma,hh),axis=1).reshape(N,1)/np.sum(gamma,axis=1).reshape(N,1)\n",
    "        \n",
    "    #print(np.sum(A,axis=0),np.sum(A,axis=1),np.sum(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.23532977, 0.26146183, 0.35918148],\n",
       "        [0.2674642 , 0.28013996, 0.31541464],\n",
       "        [0.49720603, 0.45839821, 0.32540388]]),\n",
       " array([[0.87474069, 0.12525931],\n",
       "        [0.765226  , 0.234774  ],\n",
       "        [0.30732432, 0.69267568]]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of ietrations can be control for to avoid over - fitting. \n",
    "# NoTE that we only consider one sequence of observations. Generally we have multiple sequences\n",
    "# We aim to learn the HMM model based on those sequences (by following the same procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using hmmlearn API</h2>\n",
    "\n",
    "Now that we have learned how to code for HMM and it can help us in understanding the model, we can use existing libraries to build an HMM model. We can use the same problem to see how to use the hmmlearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation  : [1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "hidden states: [2 2 0 0 2 0 2 0 1 1 2 1 1 1 2 2 0 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "pi = np.array([0,0.2,0.8])\n",
    "A  = np.array([[0.3,0.3,0.4],[0.1,0.45,0.45],[0.2,0.3,0.5]])\n",
    "B  = np.array([[1,0],[0.8,0.2],[0.3,0.7]])\n",
    "M  = 2 #(hot or cold)\n",
    "N  = 3 # snow, rain, sunshine\n",
    "T  = 20\n",
    "\n",
    "# simulate the walk based on transition probability\n",
    "# since this is generated based on actual probabilities, this state should have high probability of being observed\n",
    "s = np.random.choice(3,1,p=pi)[0]\n",
    "o = np.random.choice(2,1,p=B[s])[0]\n",
    "S = [s]\n",
    "O = [o]\n",
    "\n",
    "for t in range(T-1):\n",
    "    s = np.random.choice(3,1,p=A[s])[0]\n",
    "    o = np.random.choice(2,1,p=B[s])[0]\n",
    "    S.append(s)\n",
    "    O.append(o)\n",
    "    \n",
    "O = np.array(O)\n",
    "S = np.array(S)\n",
    "print('observation  :',O)\n",
    "print('hidden states:',S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation problem</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=3)\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.emissionprob_ = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.2877188927643e-06"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(model.score(O.reshape(1,-1)))\n",
    "# note that it exactly matches the result in the forward and backward algorithms in Section 2.1 and 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decoding problem</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0128365182460964e-10\n",
      "[2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "logprob, seq = model.decode(O.reshape(1,-1).transpose())\n",
    "print(math.exp(logprob))\n",
    "print(seq)\n",
    "# note that the sequence is exactly same as the viterbi algorithm in Section 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning problem</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(algorithm='viterbi', init_params='ste', n_components=3, n_iter=5,\n",
       "               params='ste',\n",
       "               random_state=<mtrand.RandomState object at 0x000002046923BB40>,\n",
       "               startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "               verbose=False)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remodel = hmm.MultinomialHMM(n_components=3, n_iter=5)\n",
    "remodel.fit(O.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.17705391e-03, 9.97473874e-01, 3.49071843e-04])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remodel.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38392689, 0.07199623, 0.54407688],\n",
       "       [0.32899046, 0.30694324, 0.3640663 ],\n",
       "       [0.38358656, 0.07685537, 0.53955807]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remodel.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82336472, 0.17663528],\n",
       "       [0.11720271, 0.88279729],\n",
       "       [0.8922925 , 0.1077075 ]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remodel.emissionprob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different variants of HMM. we used an example that was multinomial (discrete outcomes). The outcomes can be different distributions (for example multinomial Gaussian) or single continuous outcome (like mixture models). All these are useful models to keep in mind when considering the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
